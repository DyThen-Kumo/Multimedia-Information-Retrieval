{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from ranx import Run, fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    # Kiểm tra nếu data là một vector 1 chiều\n",
    "    if data.ndim == 1:\n",
    "        normalized_data = data / np.linalg.norm(data)\n",
    "    else:\n",
    "        # Chuẩn hóa dữ liệu nếu là mảng 2 chiều\n",
    "        normalized_data = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_lines(file_path):\n",
    "    \"\"\"Đọc toàn bộ dòng từ một file và trả về danh sách các dòng.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return [line.strip() for line in f]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} không tồn tại!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_positives, retrieved):\n",
    "  return true_positives / retrieved if retrieved != 0 else 0\n",
    "\n",
    "def recall(true_positives, relevant):\n",
    "  return true_positives / relevant if relevant != 0 else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "  return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def average_precision(relevant_items, retrieved_items):\n",
    "  '''\n",
    "  relevant_items: list các ID mà thực sự là đúng\n",
    "  retrieved_items: list các ID được truy vấn trả về\n",
    "  '''\n",
    "  relevant_items = set(relevant_items)\n",
    "  retrieved = 0\n",
    "  true_positives = 0\n",
    "  ap = 0\n",
    "\n",
    "  for i, item in enumerate(retrieved_items):\n",
    "    retrieved += 1\n",
    "    if item in relevant_items:\n",
    "      true_positives += 1\n",
    "      ap += precision(true_positives, retrieved)\n",
    "\n",
    "  return ap / len(relevant_items) if relevant_items else 0\n",
    "\n",
    "def mean_average_precision(queries):\n",
    "  aps = [average_precision(q[0], q[1]) for q in queries]\n",
    "  return np.mean(aps) if aps else 0\n",
    "\n",
    "# Hàm vẽ Precision-Recall Curve\n",
    "def plot_precision_recall_curve(queries, tittle='Precision-Recall Curve'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for idx, (relevant_items, retrieved_items) in enumerate(queries):\n",
    "        relevant_items = set(relevant_items)\n",
    "        precision_vals = []\n",
    "        recall_vals = []\n",
    "        true_positives = 0\n",
    "\n",
    "        # Tính Precision và Recall cho từng ngưỡng\n",
    "        for i, item in enumerate(retrieved_items):\n",
    "            if item in relevant_items:\n",
    "                true_positives += 1\n",
    "            p = precision(true_positives, i + 1)\n",
    "            r = recall(true_positives, len(relevant_items))\n",
    "            precision_vals.append(p)\n",
    "            recall_vals.append(r)\n",
    "\n",
    "        auc_score = auc(recall_vals, precision_vals)\n",
    "\n",
    "        # Vẽ đường Precision-Recall cho từng truy vấn\n",
    "        plt.plot(recall_vals, precision_vals, label=f'AUC = {auc_score:.2f}')\n",
    "\n",
    "    # Cấu hình biểu đồ\n",
    "    plt.title(tittle)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = r'C:\\Retrieval System\\data\\paris_120310_gt'\n",
    "data_path = r'C:\\Retrieval System\\data\\paris'\n",
    "features_path = r'C:\\Retrieval System\\features\\features_paris_clip_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6392\n",
    "retrain = True\n",
    "top_rerank = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\CS336\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "if retrain:\n",
    "    model = torch.load(r'C:\\Retrieval System\\model\\paris_clip_3.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(query_file, device='cpu'):\n",
    "    \"\"\"\n",
    "    Đọc file query, cắt ảnh theo bounding box và extract feature theo từng bb.\n",
    "    \n",
    "    :param query_file: Đường dẫn đến file query.txt\n",
    "    :param image_folder: Thư mục chứa ảnh\n",
    "    \"\"\"\n",
    "    with open(query_file, 'r') as file:\n",
    "        features = []\n",
    "        for line in file:\n",
    "            # Tách dòng query thành các phần tử\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            \n",
    "            image_name = parts[0] + '.jpg'  # Tên file ảnh\n",
    "            x1, y1, x2, y2 = map(float, parts[1:])  # Bounding box\n",
    "            \n",
    "            # Đường dẫn đầy đủ đến ảnh\n",
    "            image_path = os.path.join(data_path, image_name.split('_')[1], image_name)\n",
    "            \n",
    "            # Kiểm tra ảnh tồn tại\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Ảnh {image_name} không tồn tại trong thư mục {os.path.join(data_path, image_name.split('_')[1])}.\")\n",
    "                continue\n",
    "            \n",
    "            # Mở ảnh và cắt theo bounding box\n",
    "            with Image.open(image_path).convert('RGB') as img:\n",
    "                cropped_img = img.crop((x1, y1, x2, y2))  # Cắt ảnh\n",
    "                # Extract feature của ảnh crop\n",
    "                cropped_img = transform(cropped_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feature = model.get_image_features(cropped_img)\n",
    "                feature = feature.squeeze().cpu().numpy()\n",
    "                features.append(feature)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['defense_1', 'defense_2', 'defense_3', 'defense_4', 'defense_5', 'eiffel_1', 'eiffel_2', 'eiffel_3', 'eiffel_4', 'eiffel_5', 'invalides_1', 'invalides_2', 'invalides_3', 'invalides_4', 'invalides_5', 'louvre_1', 'louvre_2', 'louvre_3', 'louvre_4', 'louvre_5', 'moulinrouge_1', 'moulinrouge_2', 'moulinrouge_3', 'moulinrouge_4', 'moulinrouge_5', 'museedorsay_1', 'museedorsay_2', 'museedorsay_3', 'museedorsay_4', 'museedorsay_5', 'notredame_1', 'notredame_2', 'notredame_3', 'notredame_4', 'notredame_5', 'pantheon_1', 'pantheon_2', 'pantheon_3', 'pantheon_4', 'pantheon_5', 'pompidou_1', 'pompidou_2', 'pompidou_3', 'pompidou_4', 'pompidou_5', 'sacrecoeur_1', 'sacrecoeur_2', 'sacrecoeur_3', 'sacrecoeur_4', 'sacrecoeur_5', 'triomphe_1', 'triomphe_2', 'triomphe_3', 'triomphe_4', 'triomphe_5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 512)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = []\n",
    "map_qr = []\n",
    "for file in os.listdir(gt_path):\n",
    "    if 'query' in file:\n",
    "        # print(file)\n",
    "        map_qr.append(file.split('.')[0])\n",
    "        query_file = os.path.join(gt_path, file)\n",
    "        features = process_queries(query_file=query_file)\n",
    "        if len(features) == 1:\n",
    "            query.append(features[0])\n",
    "\n",
    "query = np.array(query)\n",
    "map_qr = [name[:-6] if name.endswith(\"_query\") else name for name in map_qr]\n",
    "print(map_qr)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6392, 512)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = []\n",
    "map_db = []\n",
    "for file in os.listdir(features_path):\n",
    "    file_path = os.path.join(features_path, file)\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        image_features_data = pickle.load(f) # 1 list, các phần tử là dict với image_name và features\n",
    "\n",
    "        for item in image_features_data:\n",
    "            database.append(item['features'])\n",
    "            map_db.append(item['image_name'].replace('.jpg',''))\n",
    "\n",
    "database = np.array(database)\n",
    "database.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_top_k(query, database, k=5):\n",
    "    similarity_matrix = cosine_similarity(query, database)\n",
    "\n",
    "    sorted_similarity_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1] # Sắp xếp giảm dần\n",
    "\n",
    "    # # Sắp xếp lại map_db theo thứ tự của similarity_matrix\n",
    "    # sorted_map_db = []\n",
    "    # for i in range(similarity_matrix.shape[0]):\n",
    "    #     sorted_map_db.append([map_db[j] for j in sorted_similarity_indices[i]])\n",
    "\n",
    "\n",
    "    top_k = []\n",
    "    top_k_similarities = []\n",
    "\n",
    "    for i in range(sorted_similarity_indices.shape[0]):\n",
    "        top_k_indices = sorted_similarity_indices[i][:k]\n",
    "        top_k.append([map_db[j] for j in top_k_indices])  # Lấy các đối tượng tương ứng\n",
    "        top_k_similarities.append(similarity_matrix[i][top_k_indices])  # Lấy cosine similarity\n",
    "\n",
    "    return top_k, top_k_similarities\n",
    "\n",
    "# retrieval_top_k(query=query, database=database, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_threshold(query, database, map_db, threshold=0.8):\n",
    "    # Tính toán ma trận cosine similarity\n",
    "    similarity_matrix = cosine_similarity(query, database)\n",
    "\n",
    "    # Sắp xếp các chỉ số giảm dần của cosine similarity\n",
    "    sorted_similarity_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "\n",
    "    # Khởi tạo danh sách trả về\n",
    "    filtered_map_db = []\n",
    "    filtered_similarities = []\n",
    "\n",
    "    # Duyệt qua từng query và lọc các kết quả có cosine similarity > threshold\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        # Lấy các chỉ số có độ tương đồng lớn hơn threshold\n",
    "        valid_indices = sorted_similarity_indices[i][similarity_matrix[i, sorted_similarity_indices[i]] > threshold]\n",
    "        filtered_map_db.append([map_db[j] for j in valid_indices])  # Lấy các đối tượng tương ứng\n",
    "        filtered_similarities.append(similarity_matrix[i, valid_indices])  # Lấy cosine similarity\n",
    "\n",
    "    return filtered_map_db, filtered_similarities\n",
    "\n",
    "# retrieval_threshold(query=query, database=database, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list, similarity_matrix = retrieval_top_k(query=query, database=database, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = []\n",
    "for query_name in map_qr:\n",
    "    file_ok = os.path.join(gt_path, f\"{query_name}_ok.txt\")\n",
    "    file_good = os.path.join(gt_path, f\"{query_name}_good.txt\")\n",
    "\n",
    "    # Đọc dòng từ từng file\n",
    "    lines_ok = load_file_lines(file_ok)\n",
    "    lines_good = load_file_lines(file_good)\n",
    "\n",
    "    # Gộp nội dung 2 file thành 1 danh sách\n",
    "    merged_list = lines_good + lines_ok\n",
    "\n",
    "    ground_truths.append(merged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for i in range(len(rank_list)):\n",
    "    relevant_items = ground_truths[i]\n",
    "    retrieved_items = rank_list[i]\n",
    "    queries.append((relevant_items, retrieved_items))\n",
    "    # plot_precision_recall_curve(queries=[(relevant_items, retrieved_items)], tittle=f'Precision-Recall Curve of query {i+1}')\n",
    "    # print(average_precision(relevant_items=relevant_items, retrieved_items=retrieved_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7721015140756843"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(queries=queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results_by_ranx(list_rank_list, list_similarity_matrix, method='gmnz'): # Merge kết quả của từng query.\n",
    "  # Chuyển đổi dữ liệu sang dạng dictionary\n",
    "  runs = []\n",
    "  for i, (res, sims) in enumerate(zip(list_rank_list, list_similarity_matrix)):\n",
    "      query_id = f\"query_0\"\n",
    "      run_data = {str(doc_id): score for doc_id, score in zip(res, sims)}\n",
    "      run = Run({query_id: run_data})\n",
    "      runs.append(run)\n",
    "  # Hợp nhất kết quả\n",
    "  fused_run = fuse(runs, method=method)  # Có thể điều chỉnh weights nếu cần\n",
    "\n",
    "  fused_ranking_results = {}\n",
    "\n",
    "  for query_id, docs in fused_run.to_dict().items():\n",
    "      fused_ranking_results[query_id] = list(docs.keys())  # Lấy danh sách document IDs (không cần điểm số)\n",
    "\n",
    "  return fused_ranking_results['query_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris_defense_000605',\n",
       " 'paris_defense_000592',\n",
       " 'paris_defense_000090',\n",
       " 'paris_defense_000503',\n",
       " 'paris_defense_000207',\n",
       " 'paris_defense_000094',\n",
       " 'paris_defense_000054',\n",
       " 'paris_defense_000000',\n",
       " 'paris_defense_000408',\n",
       " 'paris_defense_000259']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_list[0][:top_rerank]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
